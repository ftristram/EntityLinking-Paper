\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}

\usepackage{url}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{caption}
\usepackage[]{algorithm2e}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{multirow}
\usepackage{enumitem}

\usepackage{lipsum}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{listings}
\lstset{numbers=left,numbersep=4pt,numberstyle=\tiny\colour{gray}, basicstyle=\ttfamily\footnotesize, tabsize=2, framexleftmargin=2pt, captionpos=b, frame=none,framerule=0pt, breaklines=true, showspaces=false,showtabs=false, keywordstyle=\colour{white}, morekeywords={lemon:,lexinfo:}}

\begin{document}

\mainmatter  % start of an individual contribution

\newcommand{\acronym}{V-SEL}

%opening
\title{Vector-Similarity based Entity Linking}
\titlerunning{Vector-Similarity based Entity Linking}

\author{Felix Tristram \and Sebastian Walter \and Philipp Cimiano}
\authorrunning{Felix Tristram \and Sebastian Walter \and Philipp Cimiano}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Semantic Computing Group, CITEC, Bielefeld University}


\maketitle


\begin{abstract}


\keywords{Ontology lexicalization, DBpedia, \acronym{}}
\end{abstract}

\section{Introduction}\label{sec:introduction}
\lipsum[1-4]

\section{Approach}\label{sec:approach}
This chapter will explain the algorithm underlying the \textbf{V}ector-\textbf{S}imilarity based \textbf{E}ntity \textbf{L}inking {(\acronym )} approach. Following this paragraph basic components of the algorithm will be sketched out. In the next section the data preparation will be explained, starting with the off-line and followed by the on-line calculations. Lastly, the way candidates are scored will be discussed.

The input data for {\acronym} is a regular piece of text, for example a newspaper article (further referred to simply as \textit{input}). For a given fragment within that text the algorithm is supposed to determine the matching real world entity. In order to do so, a set of entity candidates for said fragment is needed, of which the algorithm has to pick the one which is most likely correct. Choosing one candidate over an other is achieved by computing a score for each candidate using four components:

The \textbf{tf--idf vector similarity} measures how similar the input is to the abstract of the candidate's Wikipedia entry with regards to the words that are used in both texts. The tf--idf value for a word reflects how important it is in a document (see Chapter \ref{sec:tfidf}), therefore if there is a large overlap between the important words in the input and the abstract of a candidate's Wikipedia page, the candidate is considered to be more likely correct.

The \textbf{semantic signature vector similarity} measures the overlap between the candidate's \textbf{semantic signature} (see Chapter \ref{subsec:semsig}) and the set of all candidates for the input. It stands to reason that a candidate is more likely to be the correct assignment for a given fragment if entities from its semantic signature are considered for other fragments (as related entities probably appear together).

To account for how often an anchor refers to a given entity the \textbf{candidate reference frequency} is incorporated into the score calculation. Using the word \textit{Michelangelo} as an anchor we might receive as candidates Michelangelo the Renaissance artist/engineer\footnote{\url{http://en.wikipedia.org/wiki/Michelangelo}} and Michelangelo the Ninja Turtle\footnote{\url{http://en.wikipedia.org/wiki/Michelangelo_(Teenage_Mutant_Ninja_Turtles)}}. While both are valid candidates worthy of consideration, we can extract from Wikipedia that \textit{Michelangelo} referred only 21 times to the Ninja Turtle, but 1173 times to the artist. Normalizing these candidate reference frequencies to a range of 0 to 1 (dividing by the highest frequency for each fragment) we can use them as a factor when calculating the score for a candidate.

Another variable used to increase scores for more `important' entities is their \textbf{PageRank}. Applying the same algorithms used by Google around the new millennium we can calculate a PageRank for each entity in the interlinked graph of Wikipedia entries. The candidate reference frequency and the PageRank are quite similar in what they are trying to measure, namely the universal relevance of a given entity. Experiments to gather whether the two values complement or contrast each other are described in Chapter \ref{sec:experiments}.

Note that all four components can be precomputed for each entity known to the system, which means only few computations have to be done during runtime.

\subsection{Off-line calculation}\label{sec:offlineCalc}
In this chapter the calculations which can be completed ahead of time will be discussed.

\subsection{Anchors and Candidates}\label{sec:candidatesAndAnchors}
Anchors and their entities were extracted directly from a Wikipedia XML dump\footnote{More information on downloading Wikipedia dumps here: \url{http://en.wikipedia.org/wiki/Wikipedia:Database_download}}. In these dumps a hyperlink to another page appears together like so:\\
\centerline{\tt{[[Barack Obama|President Obama]]}}\\
where {\textit{Barack Obama}\footnote{\url{http://en.wikipedia.org/wiki/Barack_Obama}} refers to an entity and \textit{President Obama} is the anchor string which appears in the page's text. After parsing all Wikipedia articles (in a chosen language) and counting the occurrences of anchor-entity pairs a text file is generated to store all the data (see Listing 
%\ref{listing:anchorExcerpt} 
for an excerpt). This file is the foundation for finding all applicable candidates for a given fragment. The script for generating the anchor file has been provided by Sebastian Walter, M. Sc., who is one of the supervisors of this thesis.

%\lstset{language={}}
%\begin{lstlisting}[caption={Excerpt from the anchor file.},label=listing:anchorExcerpt]
%(...)
%Pharos	http://dbpedia.org/resource/Hvar	17
%vulnerability	http://dbpedia.org/resource/Format_string_attack	1
%Idaho	http://dbpedia.org/resource/Idaho_Vandals_football	96
%Dorothea Lasky	http://dbpedia.org/resource/Dorothea_Lasky	 4
%Paul Butler	http://dbpedia.org/resource/Paul_Butler_(polo)	5
%(...)
%\end{lstlisting}

Note that the candidate reference frequency is calculated from the counts of occurrences within that anchor file. Once all candidates for a given fragment are identified, their counts are normalized by dividing their value by the highest such count. This yields the candidate reference frequency in a range from 0 to 1. Used as a factor in the overall scoring scheme, candidates which were more often linked through the anchor-fragment under observation receive a higher score.

\subsection{Semantic signature vector}\label{subsec:semsig}
The semantic signature for an entity is computed as described in \cite{Babelfy}, but for the graph of interlinked Wikipedia pages (each page is a vertex, each link between pages is an edge) instead of the BabelNet\footnote{\url{http://babelnet.org/}} knowledge base. The idea is to identify for each page all other pages it is closely related to. This is a two step process:

First, the edges will be assigned weights so that edges in denser parts of the graph are more important. The weight of an edge $\mathrm{weight}(v, v')$ is the number of directed triangles (directed cycles of length 3) it appears in:
$$\mathrm{weight}(v, v') := |\{(v, v', v'') : (v, v'), (v', v''), (v'', v) \in E\}| + 1$$

where $v$, $v'$ and $v''$ are vertices and $E$ is the set of all the graph's edges.

Second, random walks with restart are performed from each vertex. For a number of $n$ steps\footnote{100.000 in this implementation. More would have not been computable in a reasonable time.} the graph is explored by `walking' to a connected vertex with probability $1 - \alpha$ or to move back to the vertex of origin with probability $\alpha$. The probability of visiting the vertex $v'$ is based on the normalized weight of the connecting edge:
$$P(v'|v) = \frac{\mathrm{weight}(v, v')}{\sum _{v''\in V} \mathrm{weight}(v, v'')}$$
where V is the set of all vertices in the graph.

During the random walk for a given root vertex $v_r$, the visits to each vertex $v_i (i != r)$ are counted. The result is a frequency distribution of related vertices in which the vertices with the higher counts are interpreted as to be more relevant to the root vertex than vertices with lower counts. Vertices with a count lower than threshold $\tau$ are ignored\footnote{$\tau = 10$ for this implementation, so that $n$ and $\tau$ have the same ratio as in \cite{Babelfy}.}.

For each entity in the database we now look at its semantic signature and save the top 100 entries (i.e., the entities that were most often reached by random walks), which form the semantic signature vector (see Chapter \ref{subsubsec:vectormap} for the implementation). The amount of stored entities was arbitrarily set to 100 to fix the memory demand of the implementation. The full semantic signature could be stored, though this leads to diminishing returns as the lower the counts for an entity are, the lower its influence will be on the final score.

\subsection{tf--idf vector}\label{sec:offlineTFIDF}
The tf--idf vector for each entity is generated by picking the 100 terms from its Wikipedia page's abstract $a$ with the highest tf--idf score (see Chapter \ref{sec:tfidf} for an explanation of tf--idf and its calculation). To calculate this score for a term $t$ its tf and idf values are needed. The tf value can be obtained by simply counting all occurrences of $t$ in $a$. The idf value can be obtained by treating the set of all entities' abstracts as the corpus and computing the idf score for every occurring word. All the idf values are kept in a map, as they are needed for the on-line tf--idf compuation (see Chapter \ref{subsec:tfidfInput}).

\subsection{PageRank}\label{sec:pagerank}
The idea behind the PageRank algorithm was to estimate the relevance of a website to a search query not only by whether the search terms appear on that page, but also by how many other pages were providing a hyperlink to the website in question. As Wikipedia entries form a highly interconnected graph the same principle can be applied to them to determine the page rank of an entity local to Wikipedia.

Let $u$ be a web page. $B_u$ is the set of pages that have a hyperlink pointing to $u$. Let $N_u$ be the number of outgoing links for page $u$. A slightly simplified PageRank is then defined as such: 
$$R(u) = c \sum_{v \in B_u} \frac{R(v)}{N_v}$$
where $c$ is a normalizing factor. Notably the PageRank of a page is derived from the PageRank of its incoming links, meaning that the algorithm has to be iterated until the system reaches a steady state. The overall PageRank for a graph is supposed to be constant, but there are corner cases (like cycles) that have to be accounted for. See \cite{pageRank} for more information.

\subsection{On-line calculation}

This chapter describes the calculations that have to performed by the system during runtime.

\subsection{Finding the eligible candidates}
Since multi-token fragments are already explicitly noted in the used data sets (see Chapter \ref{sec:dataset}), {\acronym} uses a simplified algorithm for candidate identification. Candidate scoring, instead of sophisticated fragment and candidate identification, was the focus of the development of {\acronym}. But {\acronym} would undeniably benefit from an in-depth fragment recognizer and candidate identifier.

Tokens that are not part of a hand-annotated fragment, as well as single- or multi-token expressions from fragments, are simply looked up in the set of known anchors to determine their candidates. This works quite well for straightforward annotations like $Red\ Bull \rightarrow Red\_Bull$ (as it might appear in the AIDA data set), but is problematic with more obscure annotations like $Red\ Bull \rightarrow FC\_Red\_Bull\_Salzburg$ (as it might appear in the KORE50 data set). See Chapter \ref{sec:dataset} for more details.

\subsection{Semantic signature vector for input}\label{subsection:semSigVector}
To calculate the similarity score for each candidate, their semantic signature vector needs to compared to the semantic signature vector of the input. Unfortunately, the input does not have a semantic signature of its own. As a surrogate a candidate vector is generated as such: Let $m(e) \rightarrow c$ be a mapping from an entity $e$ to a count $c$. For each candidate-entity of every fragment, check whether it already appears in the mapping. If not, add it to the map and set its count to one. If it exists, increment its count by one.

Therefore a candidate's semantic signature similarity score increases as entities from their semantic signature appear more often as candidates in a given input (see Chapter \ref{subsec:VectorEvaluation} for the implementation).

\subsection{tf--idf vector for input}\label{subsec:tfidfInput}
The tf--idf vector for the input is calculated in the same fashion as for the candidates. The term frequency for every term in the input is found by counting all its occurrences, and its inverse document frequency is looked up in the idf map that was generated in Chapter \ref{sec:offlineTFIDF}. Terms that are unique to the input are ignored, as they do not affect the entity's vector score (they are multiplied by zero in the vector similarity calculation -- see Chapter \ref{sec:score computation}).

\subsection{Score computation}\label{sec:score computation}
The cosine similarity between the vectors $A$ and $B$ is calculated as follows
$$\mathrm{similarity}(A, B) = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}$$
In theory each vector needs one dimension per existing word or Wikipedia page (for tf--idf and semantic signature vectors respectively) which is unnecessary in practice. To save memory and computation time only the 100 highest scored terms or entities are saved for each entity. When calculating the dot product of two vectors there will often be entries that only occur in one vector but not the other -- for example, when word $w$ appears multiple times in a candidate's Wikipedia article's abstract, but does not appear at all in the input text. The value for dimension $w$ is then considered to be zero for the input text's tf--idf vector, therefore the value from the candidate's tf--idf vector for dimension $w$ will be multiplied by zero and does not increase the similarity between the two vectors.

There will be two similarity scores for each candidate:
\begin{enumerate}
\item The similarity $s_{\text{tf--idf}}$ between the tf--idf vectors of the input and the candidate.
\item The similarity $s_{\text{semsig}}$ between the candidate vector of the input and the semantic signature vector of the candidate.
\end{enumerate}

Though we now have four different components for computing a candidate score ($s_{\text{tf--idf}}$, $s_{\text{semsig}}$, candidate reference frequency, and PageRank), it is not obvious how to combine these components to receive the best results. In section \ref{sec:experiments} various score computation schemes are evaluated.

\section{Conclusion}\label{sec:conclusion}
\lipsum[5-7]

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
