\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}

\usepackage{url}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{caption}
\usepackage[]{algorithm2e}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{multirow}
\usepackage{enumitem}

\usepackage{lipsum}
\usepackage{todonotes}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{listings}
\lstset{numbers=left,numbersep=4pt,numberstyle=\tiny\colour{gray}, basicstyle=\ttfamily\footnotesize, tabsize=2, framexleftmargin=2pt, captionpos=b, frame=none,framerule=0pt, breaklines=true, showspaces=false,showtabs=false, keywordstyle=\colour{white}, morekeywords={lemon:,lexinfo:}}

\begin{document}

\mainmatter  % start of an individual contribution

\newcommand{\acronym}{V-SEL}



%opening
\title{Vector-Similarity based Entity Linking}
\titlerunning{Vector-Similarity based Entity Linking}

\author{Felix Tristram \and Sebastian Walter \and Philipp Cimiano}
\authorrunning{Felix Tristram \and Sebastian Walter \and Philipp Cimiano}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Semantic Computing Group, CITEC, Bielefeld University}


\maketitle


\begin{abstract}


\keywords{Ontology lexicalization, DBpedia, \acronym{}}
\end{abstract}

\section{Introduction}\label{sec:introduction}
Entity linking belongs to the field of natural language processing and is concerned with determining the identity of entities mentioned in a text. Note that entity linking is not concerned with recognizing a person as a person or a name as a name -- such a practice is the concern of a task known as named-entity recognition. Entity linking only refers to creating an association between a token in some form of input with the representation of an entity in some knowledge base.

\todo{Write introduction}

\section{Approach}\label{sec:approach}
The input data for {\acronym} is a regular piece of text, for example a newspaper article (further referred to simply as \textit{input}), in which entity-representing fragments are already identified (possibly by named-entity recognition software). A fragment from a given text is a token, or multiple successive tokens, which are representing the same entity. For a given fragment the algorithm is supposed to determine the matching real world entity. In order to do so, a set of entity candidates for this fragment is needed, of which the algorithm has to pick the one which is most likely correct. 

A fragment which represents a real world entity is called an anchor. To gather a database of anchors and their corresponding entities, data was extracted directly from a Wikipedia XML dump\footnote{More information on downloading Wikipedia dumps here: \url{http://en.wikipedia.org/wiki/Wikipedia:Database_download}}. In these dumps a hyperlink to another page appears together like so:\\
\centerline{\tt{[[Barack Obama|President Obama]]}}\\
where {\textit{Barack Obama} refers to an entity\footnote{\url{http://en.wikipedia.org/wiki/Barack_Obama}} and \textit{President Obama} is the anchor string which appears in the page's text. A map is generated by parsing all Wikipedia articles (in a chosen language) and counting the occurrences of anchor-entity pairs. This map is the foundation for finding all applicable candidates for a given fragment.

Choosing one candidate over an other is achieved by computing a score for each candidate using four components:

The \textbf{tf--idf vector similarity} measures how similar the input is to the abstract of the candidate's Wikipedia entry with regards to the words which appear in both texts. The tf--idf value for a word reflects how important it is in a document, therefore if there is a large overlap between the important words in the input and the abstract of a candidate's Wikipedia page, the candidate is considered to be more likely correct.

The tf--idf vector for each entity is generated by picking the 100 (top 100 was an arbitrary choice to limit the maximal size of each datum) terms from its Wikipedia page's abstract $a$ with the highest tf--idf score. To calculate this score for a term $t$ its tf and idf values are needed. The tf value can be obtained by simply counting all occurrences of $t$ in $a$. The idf value can be obtained by treating the set of all entities' abstracts as the corpus and computing the idf score for every occurring word.


The \textbf{semantic signature vector similarity} measures the overlap between the candidate's \textbf{semantic signature} (see Chapter \ref{subsec:semsig}) and the set of all candidates for the input. It stands to reason that a candidate is more likely to be the correct assignment for a given fragment if entities from its semantic signature are considered for other fragments (as related entities probably appear together).

To account for how often an anchor refers to a given entity the \textbf{candidate reference probability} is incorporated into the score calculation. Using the word \textit{Michelangelo} as an anchor we might receive as candidates Michelangelo the Renaissance artist/engineer\footnote{\url{http://en.wikipedia.org/wiki/Michelangelo}} and Michelangelo the Ninja Turtle\footnote{\url{http://en.wikipedia.org/wiki/Michelangelo_(Teenage_Mutant_Ninja_Turtles)}}. While both are valid candidates worthy of consideration, we can extract from Wikipedia that \textit{Michelangelo} referred only 21 times to the Ninja Turtle, but 1173 times to the artist. Turning these candidate reference frequencies into probabilities (dividing by the total number of fragment-entity links) we can use them as a factor when calculating the score for a candidate.

Another variable used to increase scores for more `important' entities is their \textbf{PageRank}. Applying the same algorithms used by Google around the new millennium we can calculate a PageRank for each entity in the interlinked graph of Wikipedia entries.

Note that all four components can be precomputed for each entity known to the system, which means only few computations have to be done during runtime.

\todo{Write tansition to component explanation}

\subsection{Semantic signature vector}\label{subsec:semsig}
The semantic signature for an entity is computed as described in \cite{Babelfy}, but for the graph of interlinked Wikipedia pages (each page is a vertex, each link between pages is an edge) instead of the BabelNet\footnote{\url{http://babelnet.org/}} knowledge base. The idea is to identify for each page all other pages it is closely related to. This is a two step process:

First, the edges will be assigned weights so that edges in denser parts of the graph become more important. The weight of an edge $\mathrm{weight}(v, v')$ is the number of directed triangles (directed cycles of length 3) it appears in:
$$\mathrm{weight}(v, v') := |\{(v, v', v'') : (v, v'), (v', v''), (v'', v) \in E\}| + 1$$

where $v$, $v'$ and $v''$ are vertices and $E$ is the set of all the graph's edges.

Second, random walks with restart are performed from each vertex (a random walk is a path that consists of succession of random steps). For a number of $n$ steps\footnote{100.000 in this implementation. More would have not been computable in a reasonable time.} the graph is explored by `walking' to a connected vertex with probability $1 - \alpha$ or to move back to the vertex of origin with probability $\alpha$. The probability of visiting the vertex $v'$ is based on the normalized weight of the connecting edge:
$$P(v'|v) = \frac{\mathrm{weight}(v, v')}{\sum _{v''\in V} \mathrm{weight}(v, v'')}$$
where V is the set of all vertices in the graph.

During the random walk for a given root vertex $v_r$, the visits to each vertex $v_i (i \neq r)$ are counted. The result is a frequency distribution of related vertices in which the vertices with the higher counts are interpreted as to be more relevant to the root vertex than vertices with lower counts. Vertices with a count lower than threshold $\tau$ are ignored\footnote{$\tau = 10$ for this implementation, so that $n$ and $\tau$ have the same ratio as in \cite{Babelfy}.}.

\subsection{PageRank}\label{sec:pagerank}
The idea behind the PageRank\cite{pageRank} algorithm was to estimate the relevance of a website to a search query not only by whether the search terms appear on that page, but also by how many other pages were providing a hyperlink to the website in question. As Wikipedia entries form a highly interconnected graph the same principle can be applied to them to determine the page rank of an entity local to Wikipedia.

Let $u$ be a web page. $B_u$ is the set of pages that have a hyperlink pointing to $u$. Let $N_u$ be the number of outgoing links for page $u$. A slightly simplified PageRank is then defined as such: 
$$R(u) = c \sum_{v \in B_u} \frac{R(v)}{N_v}$$
where $c$ is a normalizing factor. Notably the PageRank of a page is derived from the PageRank of its incoming links, meaning that the algorithm has to be iterated until the system reaches a steady state. The overall PageRank for a graph is supposed to be constant, but there are corner cases (like cycles) that have to be accounted for. 
%See \cite{pageRank} for more information.

%\subsection{On-line calculation}
%
%This chapter describes the calculations that have to performed by the system during runtime.

\subsection{Finding the eligible candidates}
Since multi-token fragments are already explicitly noted in the used data sets (see Chapter \ref{sec:dataset}), {\acronym} uses a simplified algorithm for candidate identification. Candidate scoring, instead of sophisticated fragment and candidate identification, was the focus of the development of {\acronym}. But {\acronym} would undeniably benefit from an in-depth fragment recognizer and candidate identifier.

Tokens that are not part of a hand-annotated fragment, as well as single- or multi-token expressions from fragments, are simply looked up in the set of known anchors to determine their candidates. This works quite well for straightforward annotations like $Red\ Bull \rightarrow Red\_Bull$ (as it might appear in the AIDA data set), but is problematic with more obscure annotations like $Red\ Bull \rightarrow FC\_Red\_Bull\_Salzburg$ (as it might appear in the KORE50 data set).

\subsection{Semantic signature vector for input}\label{subsection:semSigVector}
To calculate the similarity score for each candidate, their semantic signature vector needs to compared to the semantic signature vector of the input. Unfortunately, the input does not have a semantic signature of its own. As a surrogate a candidate vector is generated as such: Let $m(e) \rightarrow c$ be a mapping from an entity $e$ to a count $c$. For each candidate-entity of every fragment, check whether it already appears in the mapping. If not, add it to the map and set its count to one. If it exists, increment its count by one.

Therefore a candidate's semantic signature similarity score increases as entities from their semantic signature appear more often as candidates in a given input (see Chapter \ref{subsec:VectorEvaluation} for the implementation).

\subsection{tf--idf vector for input}\label{subsec:tfidfInput}
The tf--idf vector for the input is calculated in the same fashion as for the candidates. The term frequency for every term in the input is found by counting all its occurrences, and its inverse document frequency is looked up in the idf map that was generated in Chapter \ref{sec:offlineTFIDF}. Terms that are unique to the input are ignored, as they do not affect the entity's vector score (they are multiplied by zero in the vector similarity calculation -- see Chapter \ref{sec:score computation}).

\subsection{Score computation}\label{sec:score computation}
The cosine similarity between the vectors $A$ and $B$ is calculated as follows
$$\mathrm{similarity}(A, B) = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}$$

There will be two similarity scores for each candidate:
\begin{enumerate}
\item The similarity $s_{\text{tf--idf}}$ between the tf--idf vectors of the input and the candidate.
\item The similarity $s_{\text{semsig}}$ between the candidate vector of the input and the semantic signature vector of the candidate.
\end{enumerate}

\section{Results}\label{sec:results}

\begin{table}
\centering
\caption{Highest accuracy reached by {\acronym}, Babelfy, and DBpedia Spotlight.}
\label{table:redirects}
\begin{tabular}{ | c | c | c | c |}
    \hline
    & {\acronym} & Babelfy & DBpedia Spotlight \\ \hline
    AIDA & x\% & 66.42\%* & 49.41\%\\ \hline
    KORE50 & x\% & 47.89\% & x\%\\ \hline
\end{tabular}
\end{table}
\section{Conclusion}\label{sec:conclusion}
The experiments have shown that {\acronym} is viable approach for entity linking. On the AIDA dataset, {\acronym} performs comparably to Babelfy and better than DBpedia Spotlight. On the KORE50 dataset however Babelfy reaches a higher accuracy than {\acronym}. KORE50 was built intentionally to have a high degree of ambiguity and short sentences, which renders {\acronym 's} approach less effective: Short sentences provide fewer tokens to evaluate a tf-idf overlap, while increased numbers of candidates allow for more detrimental semantic signature vector overlaps.

{\acronym} is very fast. Evaluating the full KORE50 dataset took about 15 minutes using the Babelfy algorithm, but less than one minute with {\acronym}. Evaluation of the AIDA dataset had to be stopped after 164 hours using the Babelfy algorithm. {\acronym} evaluates the same dataset in about 45 seconds. Since the entity identifier used in this approach tries to find candidates for every token in a text, the graphs used for the Babelfy evaluation become very large, which increases evaluation time. DBpedia Spotlight was not compared with respects to speed as it was not evaluated locally.

{\acronym} works best when a fragment refers to the most common meaning of a word. For example, assignments like \textit{BSE $\rightarrow$ Bovine\_spongiform\_encephalopathy} or \textit{Franz Fischler $\rightarrow$ Franz\_Fischler} are usually correct. More ambiguous fragments get assigned correctly as well when that assignment is the most common one in the given context, for example \textit{commission $\rightarrow$ European\_Commission} in a debate of the European Parliament. Problems arise when the sought after entity is not the most common assignment for a given fragment. For example, the fragment \textit{Spanish} in \textit{Spanish Farm Minister Loyola de Palacio had earlier...} receives the assignment \textit{Spanish $\rightarrow$ Spanish\_Language}, as that token usually refers to the language. But the requested assignment was \textit{Spanish $\rightarrow$ Spain}. This particular kind of case can be avoided by introducing syntactic information to the system, for example by using an automated part of speech tagger. Using syntactic data would prevent the system of assigning a noun to an adjective-fragment.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}



























