\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}

\usepackage{url}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{caption}
\usepackage[]{algorithm2e}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{multirow}
\usepackage{enumitem}

\usepackage{lipsum}
\usepackage{todonotes}
%\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{listings}
\lstset{numbers=left,numbersep=4pt,numberstyle=\tiny\colour{gray}, basicstyle=\ttfamily\footnotesize, tabsize=2, framexleftmargin=2pt, captionpos=b, frame=none,framerule=0pt, breaklines=true, showspaces=false,showtabs=false, keywordstyle=\colour{white}, morekeywords={lemon:,lexinfo:}}

\begin{document}

\mainmatter  % start of an individual contribution

\newcommand{\acronym}{V-SEL}



%opening
\title{Vector-Similarity based Entity Linking}
\titlerunning{Vector-Similarity based Entity Linking}

\author{Felix Tristram \and Sebastian Walter \and Philipp Cimiano}
\authorrunning{Felix Tristram \and Sebastian Walter \and Philipp Cimiano}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Semantic Computing Group, CITEC, Bielefeld University}


\maketitle


\begin{abstract}


\keywords{Ontology lexicalization, DBpedia, \acronym{}}
\end{abstract}

\section{Introduction}\label{sec:introduction}
Entity linking belongs to the field of natural language processing and is concerned with determining the identity of entities mentioned in a text. Note that entity linking is not concerned with recognizing a person as a person or a name as a name -- such a practice is the concern of a task known as named-entity recognition. Entity linking only refers to creating an association between a token in some form of input with the representation of an entity in some knowledge base.

We evaluated V-SEL against the web services of FOX (Federated knOwledge eXtraction framework) and DBpedia Spotlight. FOX is a named entity recognition tool which uses AGDISTIS for entity linking. DBpedia Spotlight employs a four-staged approach which first recognizes and then links entities.

\todo[inline]{Write introduction. Add references to bibliography.}

\section{Approach}\label{sec:approach}
The input data for {\acronym} is a regular piece of text, for example a newspaper article (further referred to simply as \textit{input}), in which entity-representing fragments are already identified (possibly by named-entity recognition software). A \textit{fragment} from a given text is a token, or multiple successive tokens, which are representing the same entity. For a given fragment the algorithm is supposed to determine the matching real world entity. In order to do so, a set of entity \textit{candidates} for this fragment is needed, of which the algorithm has to pick the most likely correct one. 

A fragment which represents a real world entity is called an \textit{anchor}. To gather a database of anchors and their corresponding entities, data was extracted directly from a Wikipedia XML dump\footnote{More information on downloading Wikipedia dumps here: \url{http://en.wikipedia.org/wiki/Wikipedia:Database_download}}. In these dumps a hyperlink to another page appears together like so:\\
\centerline{\tt{[[Barack Obama|President Obama]]}}\\
where {\textit{Barack Obama} refers to an entity\footnote{\url{http://en.wikipedia.org/wiki/Barack_Obama}} and \textit{President Obama} is the anchor string which appears in the page's text. A map is generated by parsing all Wikipedia articles (in a chosen language) and counting the occurrences of anchor-entity pairs. The candidates for a given fragment are now found by looking up all entities that were referred to by the fragment within Wikipedia. 

Candidates are scored by a support vector machine. \footnote{{\acronym} is using Weka's SMO: \url{http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/SMO.html}} Four parameters are computed for each candidate, which are used as input for the SVM: tf--idf vector similarity, semantic signature vector similarity, candidate reference probability, and page rank.

% ------------ tf--idf vector similarity -------------------
\subsection{tf-idf Vector Similarity}
The \textbf{tf--idf vector similarity} measures how similar the input is to the abstract of the candidate's Wikipedia entry with regards to the words which appear in both texts. The tf--idf value for a word reflects how important it is in a document, therefore if there is a large overlap between the important words in the input and the abstract of a candidate's Wikipedia page, the candidate is considered to be more likely correct. It calculated from the term frequency (tf) and inverse document frequency (idf). $\mathrm{tf}(t, d)$ is the raw frequency of term $t$ in document $d$. $\mathrm{idf}(t, D) $ is defines as such:

$$\mathrm{idf}(t, D) = \mathrm{log}\frac{N}{|{d \in D : t \in d}|}$$

where $N$ is the number of documents in the corpus and $|{d \in D : t \in d}|$ is the number of documents $t$ appears in. The tf-idf value is the product of the two:

$$ \mathrm{tfidf}(t, d, D) = \mathrm{tf}(t, d) \times \mathrm{idf}(t, D) $$

The tf--idf vector $A$ for each entity is generated by picking the 100 (top 100 was an arbitrary choice to limit the maximal size of each datum) terms from its Wikipedia page's abstract $a$ with the highest tf--idf score. The tf value can be obtained by simply counting all occurrences of $t$ in $a$. The idf value can be obtained by treating the set of all entities' abstracts as the corpus and computing the idf score for every occurring word.

The tf--idf vector for the input $B$ is calculated in the same fashion as for the candidates. The term frequency for every term in the input is found by counting all its occurrences in the input text, and its inverse document frequency computed the same way as for each entity. Terms that are unique to the input are ignored, as they do not affect the entity's vector score (they are multiplied by zero in the vector similarity calculation).

The tf--idf vector similarity is then calculated as the cosine similarity between vectors $A$ and $B$:
$$\mathrm{tfidfVectorSimilarity}(A, B) = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}$$

% ------------ semantic signature vector similarity -------------------
\subsection{Semantic Signature Vector Similarity}
The \textbf{semantic signature vector similarity} measures the overlap between the candidate's \textbf{semantic signature} and the set of all candidates for the input. It stands to reason that a candidate is more likely to be the correct assignment for a given fragment if entities from its semantic signature are considered for other fragments (as related entities probably appear together).

The semantic signature for an entity is computed as described in \cite{Babelfy}, but for the graph of interlinked Wikipedia pages (each page is a vertex, each link between pages is an edge) instead of the BabelNet\footnote{\url{http://babelnet.org/}} knowledge base. The idea is to identify for each page all other pages it is closely related to. This is a two step process:

First, the edges will be assigned weights so that edges in denser parts of the graph become more important. The weight of an edge $\mathrm{weight}(v, v')$ is the number of directed triangles (directed cycles of length 3) it appears in:
$$\mathrm{weight}(v, v') := |\{(v, v', v'') : (v, v'), (v', v''), (v'', v) \in E\}| + 1$$

where $v$, $v'$ and $v''$ are vertices and $E$ is the set of all the graph's edges.

Second, random walks with restart are performed from each vertex (a random walk is a path that consists of succession of random steps). For a number of $n$ steps\footnote{100.000 in this implementation. More would have not been computable in a reasonable time.} the graph is explored by `walking' to a connected vertex with probability $1 - \alpha$ or to move back to the vertex of origin with probability $\alpha$. The probability of visiting the vertex $v'$ is based on the normalized weight of the connecting edge:
$$P(v'|v) = \frac{\mathrm{weight}(v, v')}{\sum\limits_{v''\in V} \mathrm{weight}(v, v'')}$$
where V is the set of all vertices in the graph.

During the random walk for a given root vertex $v_r$, the visits to each vertex $v_i (i \neq r)$ are counted. The result is a frequency distribution of related vertices in which the vertices with the higher counts are interpreted as to be more relevant to the root vertex than vertices with lower counts. Vertices with a count lower than threshold $\tau$ are ignored\footnote{$\tau = 10$ for this implementation, so that $n$ and $\tau$ have the same ratio as in \cite{Babelfy}.}.

To calculate the similarity score for each candidate, their semantic signature vector needs to compared to the semantic signature vector of the input. Unfortunately, the input does not have a semantic signature of its own. As a surrogate a candidate vector is generated as such: Let $m(e) \rightarrow c$ be a mapping from an entity $e$ to a count $c$. For each candidate-entity of every fragment, check whether it already appears in the mapping. If not, add it to the map and set its count to one. If it exists, increment its count by one.

Therefore a candidate's semantic signature similarity score increases as entities from their semantic signature appear more often as candidates in a given input.

% ------------ candidate reference probability -------------------
\subsection{Candidate Reference Probability}
To account for how often an anchor refers to a given entity the \textbf{candidate reference probability} is incorporated into the score calculation. Using the word \textit{Michelangelo} as an anchor we might receive as candidates Michelangelo the Renaissance artist/engineer\footnote{\url{http://en.wikipedia.org/wiki/Michelangelo}} and Michelangelo the Ninja Turtle\footnote{\url{http://en.wikipedia.org/wiki/Michelangelo_(Teenage_Mutant_Ninja_Turtles)}}. While both are valid candidates worthy of consideration, we can extract from Wikipedia that \textit{Michelangelo} referred only 21 times to the Ninja Turtle, but 1173 times to the artist. Turning these candidate reference frequencies into probabilities (dividing by the total number of references towards a given entity) we can use them as a factor when calculating the score for a candidate:

$$\mathrm{crp}(a, e) := \frac{\mathrm{ref}(a, e)}{{\sum\limits_{a'\in E} \mathrm{ref}(a', e)}}$$

where $\mathrm{ref}(a, e)$ is the number of references from anchor/candidate $a$ to entity $e$ and $E$ is the set of all anchors referring to $e$.

% ------------ page rank -------------------
\subsection{Page Rank}
The idea behind the PageRank\cite{pageRank} algorithm was to estimate the relevance of a website to a search query not only by whether the search terms appear on that page, but also by how many other pages were providing a hyperlink to the website in question. As Wikipedia entries form a highly interconnected graph the same principle can be applied to them to determine the page rank of an entity local to Wikipedia.

Let $u$ be a web page. $B_u$ is the set of pages that have a hyperlink pointing to $u$. Let $N_u$ be the number of outgoing links for page $u$. A slightly simplified PageRank is then defined as such: 
$$R(u) = c \sum_{v \in B_u} \frac{R(v)}{N_v}$$
where $c$ is a normalizing factor. Notably the PageRank of a page is derived from the PageRank of its incoming links, meaning that the algorithm has to be iterated until the system reaches a steady state. The overall PageRank for a graph is supposed to be constant, but there are corner cases (like cycles) that have to be accounted for. See \cite{pageRank} for more information. This implementation uses the JUNG framework\footnote{Java Universal Network/Graph Framework: \url{http://jung.sourceforge.net/}} to calculate the page rank.

\section{Results}\label{sec:results}

The systems under test were evaluated against a pre-annotated corpus for precision and recall. Precision, recall, and the derived $F_1$ score were defined as such:

$$ precision = \frac{\text{\# of correctly assigned entities}}{\text{\# of all assigned entities}} $$

$$ recall = \frac{\text{\# of correctly assigned entities}}{\text{\# of gold standard entities}} $$

$$ F_1 = 2 \cdot \frac{precision \cdot recall}{precision + recall} $$

\todo[inline]{Put up results}

\begin{table}
\centering
\caption{Results for the AIDA-YAGO dataset.}
\label{table:redirects}
\begin{tabularx}{0.8\textwidth}{  c  Y  Y  Y }
    \hline
    & FOX & DBpedia spotlight & {\acronym}  \\ 
    \hline
    Precision  & 49.27\% & 22.42\% & \textbf{59.33\%} \\
    Recall & 29.68\% & 45.95\% & \textbf{58.56\%} \\
    $F_1$ & 0.37 & 0.30 &\textbf{0.59} \\
    \hline
\end{tabularx}
\end{table}

\begin{table}
\centering
\caption{Results for the KORE50 dataset.}
\label{table:redirects}
\begin{tabularx}{0.8\textwidth}{  c  Y  Y  Y }
    \hline
    & FOX & DBpedia spotlight & {\acronym}  \\ 
    \hline
    Precision  & \textbf{30.0}\% & 21.78\% & 26.16\% \\
    Recall & 20.83\% & \textbf{34.03}\% & 23.33\% \\
    $F_1$ & 0.25 & \textbf{0.27} & 0.25 \\
    \hline
\end{tabularx}
\end{table}

\section{Conclusion}\label{sec:conclusion}
The experiments have shown that {\acronym} is viable approach for entity linking. On the AIDA dataset, {\acronym} performs comparably to Babelfy and better than DBpedia Spotlight. On the KORE50 dataset however Babelfy reaches a higher accuracy than {\acronym}. KORE50 was built intentionally to have a high degree of ambiguity and short sentences, which renders {\acronym 's} approach less effective: Short sentences provide fewer tokens to evaluate a tf-idf overlap, while increased numbers of candidates allow for more detrimental semantic signature vector overlaps.

{\acronym} is very fast. Evaluating the full KORE50 dataset took about 15 minutes using the Babelfy algorithm, but less than one minute with {\acronym}. Evaluation of the AIDA dataset had to be stopped after 164 hours using the Babelfy algorithm. {\acronym} evaluates the same dataset in about 45 seconds. Since the entity identifier used in this approach tries to find candidates for every token in a text, the graphs used for the Babelfy evaluation become very large, which increases evaluation time. DBpedia Spotlight was not compared with respects to speed as it was not evaluated locally.

{\acronym} works best when a fragment refers to the most common meaning of a word. For example, assignments like \textit{BSE $\rightarrow$ Bovine\_spongiform\_encephalopathy} or \textit{Franz Fischler $\rightarrow$ Franz\_Fischler} are usually correct. More ambiguous fragments get assigned correctly as well when that assignment is the most common one in the given context, for example \textit{commission $\rightarrow$ European\_Commission} in a debate of the European Parliament. Problems arise when the sought after entity is not the most common assignment for a given fragment. For example, the fragment \textit{Spanish} in \textit{Spanish Farm Minister Loyola de Palacio had earlier...} receives the assignment \textit{Spanish $\rightarrow$ Spanish\_Language}, as that token usually refers to the language. But the requested assignment was \textit{Spanish $\rightarrow$ Spain}. This particular kind of case can be avoided by introducing syntactic information to the system, for example by using an automated part of speech tagger. Using syntactic data would prevent the system of assigning a noun to an adjective-fragment.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}



























