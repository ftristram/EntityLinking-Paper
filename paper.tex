\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}

\usepackage{url}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{caption}
\usepackage[]{algorithm2e}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{multirow}
\usepackage{enumitem}

\usepackage{lipsum}
\usepackage{todonotes}
\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{listings}
\lstset{numbers=left,numbersep=4pt,numberstyle=\tiny\colour{gray}, basicstyle=\ttfamily\footnotesize, tabsize=2, framexleftmargin=2pt, captionpos=b, frame=none,framerule=0pt, breaklines=true, showspaces=false,showtabs=false, keywordstyle=\colour{white}, morekeywords={lemon:,lexinfo:}}

\begin{document}

\mainmatter  % start of an individual contribution

\newcommand{\acronym}{Weasel}



%opening
\title{{\acronym} meets FOX: an ensemble approach to entity linking based on vector similarity }
\titlerunning{{\acronym} meets FOX}

\author{Felix Tristram \and Sebastian Walter \and Philipp Cimiano  \and Christina Unger}
\authorrunning{Felix Tristram \and Sebastian Walter \and Philipp Cimiano  \and Christina Unger}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Semantic Computing Group, CITEC, Bielefeld University}


\maketitle


\begin{abstract}
The task of entity linking consists in disambiguating named entities occurring in textual data by linking them to an identifier in a knowledge base that represents the real-world entity they denote. We present \emph{{\acronym}}, a novel approach that is based on an ensemble of different features that is trained using a Support Vector Machine. We compare our approach to state-of-the-art tools such as FOX and DBpedia spotlight, showing that it outperforms both on the AIDA/CoNLL dataset and provides comparable results on the KORE50 dataset.

\keywords{Wikipedia, entity linking, vector similarity}
\end{abstract}

\section{Introduction}\label{sec:introduction}

The task of entity linking consists in disambiguating named entities occurring in textual data by linking them to an identifier in a knowledge base that represents the real-world entity they denote. We present \emph{{\acronym}}, a novel approach that is based on an ensemble of different features that is trained using a Support Vector Machine.  Given a named entity and a resource candidate, {\acronym} combines four features that quantify i) the likelihood that the actual name refers to the resource candidate (extracted from Wikipedia anchors), ii) the similarity of the textual context in which the name occurs to the textual context of the resource candidate (measured as the bag-of-words similarity using cosine), iii) the overlap between the semantic neighborhood signature of the resource candidate and the named entities mentioned in the larger context of the named entity, and iv) the prominence of the resource candidate as measured by its page rank in the DBpedia graph. An optimal linear model is trained using an SVM to induce an ensemble that combines these four features into an overall decision whether the named entity refers to a candidate resource or not.  

As underlying knowledge base, we use DBpedia and rely on Wikipedia pages as textual context of the resources in DBpedia to extract our textual features.

We compare our approach to state-of-the-art tools such as FOX~\cite{foxdemo} and DBpedia spotlight~\cite{spotlight}, showing that it outperforms both on the AIDA/CoNLL dataset and provides comparable results on the KORE50 dataset.

In the following section we describe the approach and then report evaluation results in Section~\ref{sec:results}.


\section{Approach}\label{sec:approach}

The input data for {\acronym} is a regular piece of text, for example a newspaper article, in which entity-representing sequences of tokens (\emph{fragments}) are already identified (we use the Stanford NER~\cite{ner}). For a given fragment, the algorithm's task is to determine the matching real-world entity. It does so by picking the most likely entity from a set of entity candidates, e.g. specified in a knowledge base, in our case DBpedia. 

A fragment which represents a real-world entity is called an \textit{anchor}. To gather a database of anchors and their corresponding entities, we extracted all anchor texts from a Wikipedia dump\footnote{\url{http://en.wikipedia.org/wiki/Wikipedia:Database_download}}. Within a dump, hyperlinks to other pages appear in the form \texttt{[[Barack Obama|President Obama]]}, 
where {\texttt{Barack Obama} refers to an entity\footnote{\url{http://en.wikipedia.org/wiki/Barack_Obama}} and \texttt{President Obama} is the anchor string which appears in the page's text. A map is generated by parsing all Wikipedia articles (in a chosen language) and counting the occurrences of anchor-entity pairs. The candidates for a given fragment are found by looking up all entities that were referred to by the fragment within Wikipedia. 

Candidates are scored by a support vector machine.\footnote{{\acronym} is using Weka's SMO: \url{http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/SMO.html}} Four features are computed for each candidate, which are used as input for the SVM: tf-idf vector similarity, semantic neighborhood signature vector similarity, candidate reference probability, and page rank. In the following we explain each of them in more detail.

% ------------ tf--idf vector similarity -------------------
\subsection{tf-idf Vector Similarity}

The \textbf{tf-idf vector similarity} measures how similar the input is to the abstract of the candidate's Wikipedia entry with regards to the words which appear in both texts. The tf-idf value for a word reflects how important it is in a document, therefore if there is a large overlap between the important words in the input and the abstract of a candidate's Wikipedia page, the candidate is considered to be more likely correct. It is calculated based on the term frequency $\mathrm{tf}(t,d)$, defined as the raw frequency of term $t$ in document $d$, and the inverse document frequency $\mathrm{idf}(t,D)$, which is defined as follows:

$$\mathrm{idf}(t, D) = \mathrm{log}\frac{N}{|{d \in D : t \in d}|}$$

Where $N$ is the number of documents in the corpus and $|{d \in D : t \in d}|$ is the number of documents $t$ appears in. The tf-idf value is the product of the two:

$$ \mathrm{tfidf}(t, d, D) = \mathrm{tf}(t, d) \times \mathrm{idf}(t, D) $$

{\acronym} computes a tf-idf vector for each entity extracted from the Wikipedia dump by picking those 100 terms $t$ from the abstract $a$ of its Wikipedia page that have the highest tf-idf score. Here the tf-value can be obtained by simply counting all occurrences of $t$ in $a$, and the idf-value can be obtained by treating the set of all entity abstracts as the corpus and computing the idf-score for every occurring word. 

The tf-idf vector for the input is then computed in the same fashion, where the term frequency for every term in the input is determined by counting its occurrences in the input text. Terms that are unique in the input are ignored, as they do not affect the entity's vector score (they are multiplied by zero in the vector similarity calculation).

The tf-idf vector similarity is then calculated as the cosine similarity between two vectors $A$ (the input terms) and $B$ (the candidate terms):
$$\mathrm{tfidfVectorSimilarity}(A, B) = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}$$

% ------------ semantic signature vector similarity -------------------
\subsection{Semantic Neighborhood Signature Vector Similarity}

The \emph{semantic neighborhood signature vector similarity} measures the overlap between the candidate's \emph{semantic neighborhood signature} and the set of all candidates for the input. The rationale is that a candidate is more likely to be the correct assignment for a given fragment if entities from its semantic neighborhood signature are considered for other fragments (as related entities probably appear together).

The semantic neighborhood signature for an entity is computed in a similar way as Babelfy~\cite{babelfy} does, except that instead of the BabelNet\footnote{\url{http://babelnet.org/}} knowledge base we use a graph of interlinked Wikipedia pages, where each page is a vertex and each link between pages is an edge. The idea is to identify for each resource all other resources it is closely related to. This is a two step process. First, the edges are assigned weights so that edges in denser parts of the graph become more important. The weight of an edge $\mathrm{weight}(v, v')$ is the number of directed triangles (directed cycles of length 3) it appears in:
$$\mathrm{weight}(v, v') := |\{(v, v', v'') : (v, v'), (v', v''), (v'', v) \in E\}| + 1$$

Where $v$, $v'$ and $v''$ are vertices and $E$ is the set of all the graph's edges.

Second, random walks with restart are performed from each vertex. % (a random walk is a path that consists of succession of random steps). 
For a number of $n$ steps (in our case 100\,000, which are still computable within a reasonable timespan) the graph is explored by `walking' to a connected vertex with probability $1 - \alpha$ or to move back to the vertex of origin with probability $\alpha$. The probability of visiting the vertex $v'$ is based on the normalized weight of the connecting edge:
$$P(v'|v) = \frac{\mathrm{weight}(v, v')}{\sum\limits_{v''\in V} \mathrm{weight}(v, v'')}$$
Where V is the set of all vertices in the graph. 

During the random walk for a given root vertex $v_r$, the visits to each vertex $v_i (i \neq r)$ are counted. The result is a frequency distribution of related vertices in which the vertices with higher counts are interpreted as to be more relevant to the root vertex than vertices with lower counts. Vertices with a count lower than threshold $\tau$ are ignored\footnote{$\tau = 10$ in our implementation, so that $n$ and $\tau$ have the same ratio as in Babelfy.}.

We compute a measure similar to the \emph{personalized page rank}~\cite{personalized_page_rank} using a random walk approach similar to the one implemented in Babelfy. This step is performed offline.

To calculate the similarity score for each candidate, their semantic neighborhood signature vector needs to compared to the semantic neighborhood signature vector of the input. Unfortunately, the input does not have a semantic neighborhood signature of its own. As a surrogate a candidate vector is generated as follows: Let $m(e) \rightarrow c$ be a mapping from an entity $e$ to a count $c$. For each candidate entity of every fragment, check whether it already appears in the mapping. If not, add it to the map and set its count to one. If it exists, increment its count by one.

Therefore a candidate's semantic neighborhood signature similarity score increases as entities from their semantic neighborhood signature appear more often as candidates in a given input.
%The top 10 elements in the semantic neighborhood signature of Barack Obama are for instance:
%\begin{enumerate}
%\item United\_States\_House\_of\_Representatives\_elections,\_2010
%\item Barack\_Obama
%\item Presidency\_of\_Barack\_Obama
%\item United\_States\_presidential\_election,\_2008
%\item Barack\_Obama\_presidential\_primary\_campaign,\_2008
%\item First\_inauguration\_of\_Barack\_Obama
%\item Democratic\_Party\_(United\_States)
%\item George\_W.\_Bush
%\item Republican\_Party\_(United\_States)
%\item Joe\_Biden
%\end{enumerate}

% ------------ candidate reference probability -------------------
\subsection{Candidate Reference Probability}

To account for how often an anchor refers to a given entity, the \emph{candidate reference probability} is incorporated into the score calculation. For example, for the anchor \textit{Michelangelo} we might receive as candidates both the artist Michelangelo\footnote{\url{http://en.wikipedia.org/wiki/Michelangelo}} and the Ninja Turtle Michelangelo\footnote{\url{http://en.wikipedia.org/wiki/Michelangelo_(Teenage_Mutant_Ninja_Turtles)}}. While both are valid candidates worthy of consideration, we can extract from Wikipedia that \textit{Michelangelo} referred only 21 times to the Ninja Turtle, but 1173 times to the artist. Turning these candidate reference frequencies into probabilities (dividing by the total number of references towards a given entity) we can use them as a factor when calculating the score for a candidate:

$$\mathrm{crp}(a, e) := \frac{\mathrm{ref}(a, e)}{{\sum\limits_{a'\in E} \mathrm{ref}(a', e)}}$$

Where $\mathrm{ref}(a, e)$ is the number of references from anchor/candidate $a$ to entity $e$ and $E$ is the set of all anchors referring to $e$.

% ------------ page rank -------------------
\subsection{Page Rank}

The idea behind the PageRank~\cite{pageRank} algorithm is to estimate the relevance of a website to a search query not only by whether the search terms appear on that page, but also by how many other pages were providing a hyperlink to the website in question. As DBpedia entries form a highly interconnected graph, the same principle can be applied in order to determine the page rank of a resource in DBpedia.

Let $u$ be a web page. $B_u$ is the set of pages that have a hyperlink pointing to $u$. Let $N_u$ be the number of outgoing links for page $u$. A slightly simplified PageRank is then defined as follows: 
$$R(u) = c \sum_{v \in B_u} \frac{R(v)}{N_v}$$
Where $c$ is a normalizing factor. Notably the PageRank of a page is derived from the PageRank of its incoming links, meaning that the algorithm has to be iterated until the system reaches a steady state. The overall PageRank for a graph is supposed to be constant, but there are corner cases (like cycles) that have to be accounted for; see \cite{pageRank} for more information. Our implementation uses the JUNG framework\footnote{Java Universal Network/Graph Framework: \url{http://jung.sourceforge.net/}}.

\section{Results}\label{sec:results}

We evaluate our entity linking approach on two datasets, AIDA/CoNLL and KORE50, and compare its results with two other state-of-the-art entity linking approaches, AGDISTIS \cite{agdistis} as part of FOX\footnote{\url{http://aksw.org/Projects/FOX.html}}, and DBpedia Spotlight\footnote{\url{http://spotlight.dbpedia.org}},  
in terms of precision and recall. Precision, recall, and the derived $F_1$ score were defined as follows:

$$Precision = \frac{\text{\# of correctly assigned entities}}{\text{\# of all assigned entities}}$$

$$Recall = \frac{\text{\# of correctly assigned entities}}{\text{\# of gold standard entities}}$$

$$F_1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}$$

Precision, Recall and F-Measure are micro-averaged over all named entities. Documents in which more named entities occur have thus a larger impact on the scores.

The quality of the NER used to find entities poses an upper bound on the achievable entity linking scores. For the AIDA/CoNLL dataset the highest achievable linking precision is 0.79, while the highest achievable recall is 0.98 (max. $F_1$: 0.88). For the KORE50 dataset the values are 0.93 and 0.88 respectively (max. $F_1$: 0.90).

Table~\ref{table:redirects1} shows the evaluation results. 
\acronym{} outperforms both FOX and DBpedia Spotlight on the AIDA/CoNLL dataset and achieves slightly worse but comparable results on the KORE50 dataset. 
The latter is due to the fact that KORE50 was designed to have a high degree of ambiguity and short sentences, which renders {\acronym 's} approach less effective: Short sentences provide fewer tokens to evaluate a tf-idf overlap, while increased numbers of candidates allow for more detrimental semantic neighborhood signature vector overlaps.

\begin{table}[t]
\begin{center}
\begin{tabularx}{\textwidth}{ c Y Y Y Y Y Y}
\toprule
& \multicolumn{3}{c}{AIDA/CoNLL}  
& \multicolumn{3}{c}{KORE50}\\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& Precision & Recall & $F_1$ & Precision & Recall & $F_1$ \\ 
\midrule
FOX & 0.49 & 0.30 & 0.37 & \textbf{0.30} & 0.21 &  0.25 \\
DBpedia Spotlight & 0.22 & 0.46 & 0.30 & 0.22 & \textbf{0.34} & \textbf{0.27} \\
{\acronym} & \textbf{0.59} & \textbf{0.59} & \textbf{0.59} & 0.26 & 0.23 &  0.25 \\
\bottomrule
\end{tabularx}
\end{center}
\caption{Results for the AIDA/CoNLL and KORE50 datasets.}
\label{table:redirects1}
\end{table}

{\acronym} works best when a fragment refers to the most common meaning of a word. For example, assignments like \textit{BSE} $\rightarrow$ \texttt{Bovine\_spongiform\_encephalopathy} are usually correct. More ambiguous fragments get assigned correctly as well when that assignment is the most common one in the given context, for example \textit{commission} $\rightarrow$ \texttt{European\_Commission} in a debate of the European Parliament. Problems arise when the target entity is not the most common assignment for a given fragment. For example, the fragment \textit{Spanish} in \textit{Spanish Farm Minister Loyola de Palacio} leads to the assignment \textit{Spanish} $\rightarrow$ \texttt{Spanish\_Language}, as the fragment usually refers to the language, whereas the gold assignment is \textit{Spanish} $\rightarrow$ \texttt{Spain}. % This particular kind of case can be avoided by introducing syntactic information to the system, for example by using an automated part-of-speech tagger that can prevent the system of assigning a noun to an adjective fragment.


\section{Conclusion}\label{sec:conclusion}

We have presented {\acronym}, a new entity linking approach that exploits an ensemble of features that are linearly combined using a Support Vector Machine. We have shown that the approach works very well when the contexts are longer, clearly outperforming FOX and DBpedia Spotlight on the AIDA CoNLL dataset. On the KORE50 dataset where contexts are shorter, it achieves comparable results to these approaches. In future work, we will release a service that can be publicly accessed using a similar interface to DBpedia Spotlight and FOX. We will also extend the approach to other languages and perform a more systematic evaluation using the GERBIL framework \cite{gerbil}.

%Ricardo Usbeck, Michael Röder, Axel-Cyrille Ngonga Ngomo, Ciro Baron, Andreas Both, Martin Brümmer, Diego Ceccarelli, Marco Cornolti, Didier Cherix, Bernd Eickmann, Paolo Ferragina, Christiane Lemke, Andrea Moro, Roberto Navigli, Francesco Piccinno, Giuseppe Rizzo, Harald Sack, René Speck, Raphaël Troncy, Jörg Waitelonis, Lars Wesemann:
%GERBIL: General Entity Annotator Benchmarking Framework. WWW 2015: 1133-1143

\section*{Acknowledgment}
This work was supported by the Cluster of Excellence Cognitive Interaction Technology \emph{CITEC} (EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG).


\bibliographystyle{plain}
\bibliography{biblio}

\end{document}



























