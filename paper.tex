\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}

\usepackage{url}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{caption}
\usepackage[]{algorithm2e}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{multirow}
\usepackage{enumitem}

\usepackage{lipsum}
\usepackage{todonotes}
%\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{listings}
\lstset{numbers=left,numbersep=4pt,numberstyle=\tiny\colour{gray}, basicstyle=\ttfamily\footnotesize, tabsize=2, framexleftmargin=2pt, captionpos=b, frame=none,framerule=0pt, breaklines=true, showspaces=false,showtabs=false, keywordstyle=\colour{white}, morekeywords={lemon:,lexinfo:}}

\begin{document}

\mainmatter  % start of an individual contribution

\newcommand{\acronym}{V-SEL}



%opening
\title{Vector Similarity based Entity Linking}
\titlerunning{Vector Similarity based Entity Linking}

\author{Felix Tristram \and Christina Unger \and Sebastian Walter \and Philipp Cimiano}
\authorrunning{Felix Tristram \and Christina Unger \and Sebastian Walter \and Philipp Cimiano}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Semantic Computing Group, CITEC, Bielefeld University}


\maketitle


\begin{abstract}
\textbf{TODO}

\keywords{Wikipedia, entity linking, vector similarity}
\end{abstract}

\section{Introduction}\label{sec:introduction}

\emph{Entity linking} is the task of determining the identity of entities mentioned in a text. It usually follows a step of \emph{named entity recognition}, which identifies tokens that represent named entities and possibly classifies them w.r.t. predefined categories such as persons or locations. Entity linking therefore only refers to creating an association between a given token in textual input with the representation of an entity in some knowledge base.

We present an approach to entity linking based on vector similarity (V-SEL), which can compete with existing state-of-the-art entity linking frameworks in terms of accuracy, and beats them significantly in terms of speed. 
In the following section we describe the approach, and then report evaluation results in Section~\ref{sec:results}.


\section{Approach}\label{sec:approach}

The input data for {\acronym} is a regular piece of text, for example a newspaper article, in which entity-representing sequences of tokens (\emph{fragments}) are already identified (possibly by a named entity recognition tool). For a given fragment, the algorithm's task is to determine the matching real-world entity. It does so by picking the most likely entity from a set of entity candidates, e.g. specified in a knowledge base, in our case Wikipedia. 

A fragment which represents a real-world entity is called an \textit{anchor}. To gather a database of anchors and their corresponding entities, we extracted all anchor texts from a Wikipedia dump\footnote{\url{http://en.wikipedia.org/wiki/Wikipedia:Database_download}}, in which hyperlinks to other pages appear in the form \texttt{[[Barack Obama|President Obama]]}, 
where {\texttt{Barack Obama} refers to an entity\footnote{\url{http://en.wikipedia.org/wiki/Barack_Obama}} and \texttt{President Obama} is the anchor string which appears in the page's text. A map is generated by parsing all Wikipedia articles (in a chosen language) and counting the occurrences of anchor-entity pairs. The candidates for a given fragment are now found by looking up all entities that were referred to by the fragment within Wikipedia. 

Candidates are scored by a support vector machine.\footnote{{\acronym} is using Weka's SMO: \url{http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/SMO.html}} Four parameters are computed for each candidate, which are used as input for the SVM: tf-idf vector similarity, semantic signature vector similarity, candidate reference probability, and page rank. In the following we explain each of them in more detail.

% ------------ tf--idf vector similarity -------------------
\subsection{tf-idf Vector Similarity}

The \textbf{tf-idf vector similarity} measures how similar the input is to the abstract of the candidate's Wikipedia entry with regards to the words which appear in both texts. The tf-idf value for a word reflects how important it is in a document, therefore if there is a large overlap between the important words in the input and the abstract of a candidate's Wikipedia page, the candidate is considered to be more likely correct. It is calculated based on the term frequency $\mathrm{tf}(t,d)$, defined as the raw frequency of term $t$ in document $d$, and the inverse document frequency $\mathrm{idf}(t,D)$, which is defines as follows:

$$\mathrm{idf}(t, D) = \mathrm{log}\frac{N}{|{d \in D : t \in d}|}$$

Where $N$ is the number of documents in the corpus and $|{d \in D : t \in d}|$ is the number of documents $t$ appears in. The tf-idf value is the product of the two:

$$ \mathrm{tfidf}(t, d, D) = \mathrm{tf}(t, d) \times \mathrm{idf}(t, D) $$

V-SEL computes a tf-idf vector for each entity extracted from the Wikipedia dump by picking those 100 terms $t$ from the abstract $a$ of its Wikipedia page that have the highest tf-idf score. Here the tf-value can be obtained by simply counting all occurrences of $t$ in $a$, and the idf-value can be obtained by treating the set of all entity abstracts as the corpus and computing the idf-score for every occurring word. 

The tf-idf vector for the input is then computed in the same fashion, where the term frequency for every term in the input is determined by counting its occurrences in the input text. Terms that are unique in the input are ignored, as they do not affect the entity's vector score (they are multiplied by zero in the vector similarity calculation).

The tf-idf vector similarity is then calculated as the cosine similarity between two vectors $A$ (the input term) and $B$ (the candidate term):
$$\mathrm{tfidfVectorSimilarity}(A, B) = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}$$

% ------------ semantic signature vector similarity -------------------
\subsection{Semantic Signature Vector Similarity}

The \emph{semantic signature vector similarity} measures the overlap between the candidate's \emph{semantic signature} and the set of all candidates for the input. The rationale is that a candidate is more likely to be the correct assignment for a given fragment if entities from its semantic signature are considered for other fragments (as related entities probably appear together).

The semantic signature for an entity is computed in the same ways as Babelfy \cite{Babelfy} does, except that instead of the BabelNet\footnote{\url{http://babelnet.org/}} knowledge base we use a graph of interlinked Wikipedia pages, where each page is a vertex and each link between pages is an edge. The idea is to identify for each page all other pages it is closely related to. This is a two step process. First, the edges are assigned weights so that edges in denser parts of the graph become more important. The weight of an edge $\mathrm{weight}(v, v')$ is the number of directed triangles (directed cycles of length 3) it appears in:
$$\mathrm{weight}(v, v') := |\{(v, v', v'') : (v, v'), (v', v''), (v'', v) \in E\}| + 1$$

Where $v$, $v'$ and $v''$ are vertices and $E$ is the set of all the graph's edges.

Second, random walks with restart are performed from each vertex. % (a random walk is a path that consists of succession of random steps). 
For a number of $n$ steps (in our case 100\,000, which are still computable within a reasonable timespan) the graph is explored by `walking' to a connected vertex with probability $1 - \alpha$ or to move back to the vertex of origin with probability $\alpha$. The probability of visiting the vertex $v'$ is based on the normalized weight of the connecting edge:
$$P(v'|v) = \frac{\mathrm{weight}(v, v')}{\sum\limits_{v''\in V} \mathrm{weight}(v, v'')}$$
Where V is the set of all vertices in the graph.

During the random walk for a given root vertex $v_r$, the visits to each vertex $v_i (i \neq r)$ are counted. The result is a frequency distribution of related vertices in which the vertices with the higher counts are interpreted as to be more relevant to the root vertex than vertices with lower counts. Vertices with a count lower than threshold $\tau$ are ignored\footnote{$\tau = 10$ in our implementation, so that $n$ and $\tau$ have the same ratio as in Babelfy.}.

To calculate the similarity score for each candidate, their semantic signature vector needs to compared to the semantic signature vector of the input. Unfortunately, the input does not have a semantic signature of its own. As a surrogate a candidate vector is generated as follows: Let $m(e) \rightarrow c$ be a mapping from an entity $e$ to a count $c$. For each candidate entity of every fragment, check whether it already appears in the mapping. If not, add it to the map and set its count to one. If it exists, increment its count by one.

Therefore a candidate's semantic signature similarity score increases as entities from their semantic signature appear more often as candidates in a given input.

% ------------ candidate reference probability -------------------
\subsection{Candidate Reference Probability}

To account for how often an anchor refers to a given entity, the \emph{candidate reference probability} is incorporated into the score calculation. For example, for the anchor \textit{Michelangelo} we might receive as candidates both the artist Michelangelo\footnote{\url{http://en.wikipedia.org/wiki/Michelangelo}} and the Ninja Turtle Michelangelo\footnote{\url{http://en.wikipedia.org/wiki/Michelangelo_(Teenage_Mutant_Ninja_Turtles)}}. While both are valid candidates worthy of consideration, we can extract from Wikipedia that \textit{Michelangelo} referred only 21 times to the Ninja Turtle, but 1173 times to the artist. Turning these candidate reference frequencies into probabilities (dividing by the total number of references towards a given entity) we can use them as a factor when calculating the score for a candidate:

$$\mathrm{crp}(a, e) := \frac{\mathrm{ref}(a, e)}{{\sum\limits_{a'\in E} \mathrm{ref}(a', e)}}$$

Where $\mathrm{ref}(a, e)$ is the number of references from anchor/candidate $a$ to entity $e$ and $E$ is the set of all anchors referring to $e$.

% ------------ page rank -------------------
\subsection{Page Rank}

The idea behind the PageRank \cite{pageRank} algorithm is to estimate the relevance of a website to a search query not only by whether the search terms appear on that page, but also by how many other pages were providing a hyperlink to the website in question. As Wikipedia entries form a highly interconnected graph, the same principle can be applied in order to determine the page rank of an entity local to Wikipedia.

Let $u$ be a web page. $B_u$ is the set of pages that have a hyperlink pointing to $u$. Let $N_u$ be the number of outgoing links for page $u$. A slightly simplified PageRank is then defined as follows: 
$$R(u) = c \sum_{v \in B_u} \frac{R(v)}{N_v}$$
Where $c$ is a normalizing factor. Notably the PageRank of a page is derived from the PageRank of its incoming links, meaning that the algorithm has to be iterated until the system reaches a steady state. The overall PageRank for a graph is supposed to be constant, but there are corner cases (like cycles) that have to be accounted for; see \cite{pageRank} for more information. Our implementation uses the JUNG framework\footnote{Java Universal Network/Graph Framework: \url{http://jung.sourceforge.net/}}.

\section{Results}\label{sec:results}

We evaluate our entity linking approach on two datasets, AIDA-YAGO and KOR50, and compare its results with two other state-of-the-art entity linking approaches, AGDISTIS \cite{agdistis} as part of FOX\footnote{\url{http://aksw.org/Projects/FOX.html}}, and DBpedia Spotlight\footnote{\url{http://spotlight.dbpedia.org}} \cite{spotlight},  
in terms of precision and recall. Precision, recall, and the derived $F_1$ score were defined as follows:

$$ Precision = \frac{\text{\# of correctly assigned entities}}{\text{\# of all assigned entities}} $$

$$ Recall = \frac{\text{\# of correctly assigned entities}}{\text{\# of gold standard entities}} $$

$$ F_1 = 2 \cdot \frac{precision \cdot recall}{precision + recall} $$

Tables~\ref{table:redirects1} and~\ref{table:redirects2} shows the evaluation results. 
\acronym{} outperforms both FOX and DBpedia Spotlight on the AIDA-YAGO dataset and achieves slightly worse but comparable results on the KOR50 dataset. 
The latter is due to the fact that KORE50 was designed to have a high degree of ambiguity and short sentences, which renders {\acronym 's} approach less effective: Short sentences provide fewer tokens to evaluate a tf-idf overlap, while increased numbers of candidates allow for more detrimental semantic signature vector overlaps.

One of the biggest strengths of {\acronym} is its speed. 
Running it on the full datasets took less than one minute for KORE50 and about 45 seconds for AIDA. 
As comparison, running Bablfy on the KOR50 dataset takes about 15 minutes, and running it on AIDA had to be stopped after 164 hours.

\begin{table}[t]
\begin{center}
\begin{tabularx}{0.8\textwidth}{  c  Y  Y  Y }
    \hline
    & FOX & DBpedia Spotlight & {\acronym}  \\ 
    \hline
    Precision  & 0.49 & 0.22 & \textbf{0.59} \\
    Recall & 0.30 & 0.46 & \textbf{0.59} \\
    $F_1$ & 0.37 & 0.30 & \textbf{0.59} \\
    \hline
\end{tabularx}
\end{center}
\caption{Results for the AIDA-YAGO dataset.}
\label{table:redirects1}
\end{table}

\begin{table}[t]
\begin{center}
\begin{tabularx}{0.8\textwidth}{  c  Y  Y  Y }
    \hline
    & FOX & DBpedia Spotlight & {\acronym}  \\ 
    \hline
    Precision  & \textbf{0.30} & 0.22 & 0.26 \\
    Recall & 0.21 & \textbf{0.34} & 0.23 \\
    $F_1$ & 0.25 & \textbf{0.27} & 0.25 \\
    \hline
\end{tabularx}
\end{center}
\caption{Results for the KORE50 dataset.}
\label{table:redirects2}
\end{table}

{\acronym} works best when a fragment refers to the most common meaning of a word. For example, assignments like \textit{BSE} $\rightarrow$ \texttt{Bovine\_spongiform\_encephalopathy} are usually correct. More ambiguous fragments get assigned correctly as well when that assignment is the most common one in the given context, for example \textit{commission} $\rightarrow$ \texttt{European\_Commission} in a debate of the European Parliament. Problems arise when the target entity is not the most common assignment for a given fragment. For example, the fragment \textit{Spanish} in \textit{Spanish Farm Minister Loyola de Palacio} leads to the assignment \textit{Spanish} $\rightarrow$ \texttt{Spanish\_Language}, as the fragment usually refers to the language, whereas the gold assignment is \textit{Spanish} $\rightarrow$ \texttt{Spain}. % This particular kind of case can be avoided by introducing syntactic information to the system, for example by using an automated part-of-speech tagger that can prevent the system of assigning a noun to an adjective fragment.


\section{Conclusion}\label{sec:conclusion}

\textbf{TODO}

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}



























